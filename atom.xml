<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>John&#39;s TBlog</title>
  
  
  <link href="https://johnnn765.github.io/atom.xml" rel="self"/>
  
  <link href="https://johnnn765.github.io/"/>
  <updated>2022-10-08T04:34:36.452Z</updated>
  <id>https://johnnn765.github.io/</id>
  
  <author>
    <name>John ZHANG</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>DataStructure Vol.2 数组队列模拟 Array Queues Demo</title>
    <link href="https://johnnn765.github.io/2022/10/08/DataStructure2/"/>
    <id>https://johnnn765.github.io/2022/10/08/DataStructure2/</id>
    <published>2022-10-08T04:15:52.000Z</published>
    <updated>2022-10-08T04:34:36.452Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://pd.daffodilvarsity.edu.bd/web/image/op.course/243/image_1920"></p><h1 id="DataStructure-Vol-2-数组队列模拟-Array-Queues-Demo"><a href="#DataStructure-Vol-2-数组队列模拟-Array-Queues-Demo" class="headerlink" title="DataStructure Vol.2 数组队列模拟 Array Queues Demo"></a>DataStructure Vol.2 数组队列模拟 Array Queues Demo</h1><p>队列是<strong>有序</strong>的，可以用<strong>数组</strong>来实现，而且队列遵循一个<strong>先入先出</strong>的原则，即存入队列的数据，要先取出，后存入的要后取出。</p><p>1、队列是有序的，若使用数组的结构来存储队列的数据，首先要用 maxsize 来指定队列的最大容量。</p><p>2、因为队列的输出、输入是分别从前后端来处理，因此需要两个变量 front 及 rear 分别记录队列前后端的下标，front 会随着数据输出而改变，而 rear 则是随着数据输入而改变，如果用C语言中的指针描述就是队首指针与队尾指针。</p><h3 id="数组队列模拟示意图"><a href="#数组队列模拟示意图" class="headerlink" title="数组队列模拟示意图"></a>数组队列模拟示意图</h3><p><img src="https://cs.sumu.cc/img/notes/20210331220809.png"></p><h3 id="Java实现代码"><a href="#Java实现代码" class="headerlink" title="Java实现代码"></a>Java实现代码</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br></pre></td><td class="code"><pre><code class="hljs Java"><span class="hljs-keyword">import</span> java.util.Scanner;<br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">Array_queues_demo</span>&#123;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> &#123;<br>        <span class="hljs-comment">// 测试代码</span><br>        <span class="hljs-comment">// 创建模拟队列</span><br>        <span class="hljs-type">ArrayQueue</span> <span class="hljs-variable">test_ArrayQueue</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">ArrayQueue</span>(<span class="hljs-number">3</span>);<br>        <span class="hljs-type">char</span> <span class="hljs-variable">key</span> <span class="hljs-operator">=</span> <span class="hljs-string">&#x27; &#x27;</span>; <span class="hljs-comment">// 接受用户的输入</span><br>        <span class="hljs-type">Scanner</span> <span class="hljs-variable">scanner</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Scanner</span>(System.in); <span class="hljs-comment">// 创建输入扫描器</span><br><br>        <span class="hljs-type">boolean</span> <span class="hljs-variable">loop</span> <span class="hljs-operator">=</span> <span class="hljs-literal">true</span>;<br>        <span class="hljs-keyword">while</span>(loop) &#123;<br>            System.out.println(<span class="hljs-string">&quot;键入s显示队列&quot;</span>);<br>            System.out.println(<span class="hljs-string">&quot;键入e退出程序&quot;</span>);<br>            System.out.println(<span class="hljs-string">&quot;键入a添加数据到数列&quot;</span>);<br>            System.out.println(<span class="hljs-string">&quot;键入g从队列取出数据&quot;</span>);<br>            System.out.println(<span class="hljs-string">&quot;键入h查看队列头数据&quot;</span>);<br><br>            key = scanner.next().charAt(<span class="hljs-number">0</span>); <span class="hljs-comment">//接受一个输入字符</span><br>            <span class="hljs-keyword">switch</span> (key) &#123;<br>                <span class="hljs-keyword">case</span> <span class="hljs-string">&#x27;s&#x27;</span>:<br>                    test_ArrayQueue.showQueue();<br>                    <span class="hljs-keyword">break</span>;<br>                <span class="hljs-keyword">case</span> <span class="hljs-string">&#x27;a&#x27;</span>:<br>                    System.out.println(<span class="hljs-string">&quot;输入一个数据&quot;</span>);<br>                    <span class="hljs-type">int</span> <span class="hljs-variable">value</span> <span class="hljs-operator">=</span> scanner.nextInt();<br>                    test_ArrayQueue.addQueue(value);<br>                    <span class="hljs-keyword">break</span>;<br>                <span class="hljs-keyword">case</span> <span class="hljs-string">&#x27;g&#x27;</span>:<br>                    <span class="hljs-keyword">try</span> &#123;<br>                    <span class="hljs-comment">// getQueue方法中有抛出错误，利用try_catch解决抛出异常的情况</span><br>                        <span class="hljs-type">int</span> <span class="hljs-variable">result</span> <span class="hljs-operator">=</span> test_ArrayQueue.getQueue();<br>                        System.out.printf(<span class="hljs-string">&quot;取出的数据是%d\n&quot;</span>, result);<br>                    &#125; <span class="hljs-keyword">catch</span> (Exception e) &#123;<br>                        <span class="hljs-comment">// <span class="hljs-doctag">TODO:</span> handle exception</span><br>                        System.out.printf(e.getMessage());<br>                    &#125;<br>                    <span class="hljs-keyword">break</span>;<br>                <span class="hljs-keyword">case</span> <span class="hljs-string">&#x27;h&#x27;</span>:<br>                    <span class="hljs-keyword">try</span> &#123;<br>                    <span class="hljs-comment">// headQueuePeeking法中有抛出错误，利用try_catch解决抛出异常的情况</span><br>                        <span class="hljs-type">int</span> <span class="hljs-variable">result</span> <span class="hljs-operator">=</span> test_ArrayQueue.headQueuePeeking();<br>                        System.out.printf(<span class="hljs-string">&quot;队列头部的数据是%d\n&quot;</span>, result);<br>                    &#125; <span class="hljs-keyword">catch</span> (Exception e) &#123;<br>                        <span class="hljs-comment">// <span class="hljs-doctag">TODO:</span> handle exception</span><br>                        System.out.printf(e.getMessage());<br>                    &#125;<br>                    <span class="hljs-keyword">break</span>;<br>                <span class="hljs-keyword">default</span>:<br>                    <span class="hljs-keyword">break</span>;<br>            &#125;<br>        &#125;<br>    &#125;<br>&#125;<br><br><span class="hljs-comment">//使用数组模拟队列, 编写一个ArrayQueue的类</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ArrayQueue</span> &#123;<br>    <span class="hljs-keyword">private</span> <span class="hljs-type">int</span> maxSize; <span class="hljs-comment">// 表示队列的最大容量数量</span><br>    <span class="hljs-keyword">private</span> <span class="hljs-type">int</span> front; <span class="hljs-comment">// 定义队列头部</span><br>    <span class="hljs-keyword">private</span> <span class="hljs-type">int</span> rear; <span class="hljs-comment">// 定义队列尾部</span><br>    <span class="hljs-keyword">private</span> <span class="hljs-type">int</span>[] arr; <span class="hljs-comment">// 定义用于存放数据的模拟数组</span><br><br>    <span class="hljs-comment">// 创建队列的构造器</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-title function_">ArrayQueue</span><span class="hljs-params">(<span class="hljs-type">int</span> arrMaxSize)</span> &#123;<br>        maxSize = arrMaxSize;<br>        arr = <span class="hljs-keyword">new</span> <span class="hljs-title class_">int</span>[arrMaxSize];<br>        <span class="hljs-comment">// 利用传入的arrMaxSize参数构建模拟队列数组</span><br>        front = -<span class="hljs-number">1</span>;<br>        <span class="hljs-comment">// front指向队列中队列头部数据的前一个位置</span><br>        rear = -<span class="hljs-number">1</span>;<br>        <span class="hljs-comment">// 指向队列尾部的数据</span><br>    &#125;<br><br>    <span class="hljs-comment">// 判断队列是否已经满</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-type">boolean</span> <span class="hljs-title function_">isFull</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">return</span> rear == maxSize - <span class="hljs-number">1</span>;<br>        <span class="hljs-comment">// maxSize - 1 代表模拟队列数组的最后一个数据位置（单向队列）</span><br>    &#125;<br><br>    <span class="hljs-comment">// 判断队列是否为空</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-type">boolean</span> <span class="hljs-title function_">isEmpty</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">return</span> rear == front;<br>        <span class="hljs-comment">// 只要队头和队尾指针重叠即认为队列为空状态</span><br>    &#125;<br><br>    <span class="hljs-comment">// 添加数据到队列中</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">addQueue</span><span class="hljs-params">(<span class="hljs-type">int</span> n)</span> &#123;<br>        <span class="hljs-comment">// 首先判断队列是否已满</span><br>        <span class="hljs-keyword">if</span>(isFull()) &#123;<br>            System.out.println(<span class="hljs-string">&quot;队列已满，无法入列&quot;</span>);<br>            <span class="hljs-keyword">return</span>;<br>            <span class="hljs-comment">// 队列已满则直接跳出判断</span><br>        &#125;<br>        rear++; <span class="hljs-comment">// 添加数据到队列rear指针后移</span><br>        arr[rear] = n;<br>    &#125;<br><br>    <span class="hljs-comment">// 数据出队列</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-type">int</span> <span class="hljs-title function_">getQueue</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-comment">// 首先判断队列是否为空</span><br>        <span class="hljs-keyword">if</span>(isEmpty()) &#123;<br>            <span class="hljs-comment">// 抛出异常处理</span><br>            <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">RuntimeException</span>(<span class="hljs-string">&quot;队列为空，不能取出&quot;</span>);<br>        &#125;<br>        front++; <span class="hljs-comment">// 基于front的定义在返回取出的数据之前需要对front指针后移</span><br>        <span class="hljs-keyword">return</span> arr[front];<br>    &#125;<br><br>    <span class="hljs-comment">// 显示队列的所有数据</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">showQueue</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-comment">// 首先仍然判断队列是否为空</span><br>        <span class="hljs-keyword">if</span>(isEmpty()) &#123;<br>            System.out.println(<span class="hljs-string">&quot;队列为空，无法显示队列&quot;</span>);<br>            <span class="hljs-keyword">return</span>;<br>            <span class="hljs-comment">// 队列为空无法显示队列则直接跳出判断</span><br>        &#125;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; i&lt;arr.length; i++) &#123;<br>            System.out.printf(<span class="hljs-string">&quot;arr[%d]=%d\n&quot;</span>, i, arr[i]);<br>        &#125;<br>    &#125;<br><br>    <span class="hljs-comment">// 显示队列的头数据</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-type">int</span> <span class="hljs-title function_">headQueuePeeking</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">if</span>(isEmpty()) &#123;<br>            <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">RuntimeException</span>(<span class="hljs-string">&quot;队列为空，不能取出&quot;</span>);<br>            <span class="hljs-comment">// 队列为空无法显示队列则直接跳出判断</span><br>        &#125;<br>        <span class="hljs-keyword">return</span> arr[front+<span class="hljs-number">1</span>];<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>__本文作者__：Johnnn765<br />__本文地址__： <a href="https://johnnn765.github.io/2022/10/08/DataStructure2/">https://johnnn765.github.io/2022/10/08/DataStructure2/</a> <br />__版权声明__：转载请注明出处！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://pd.daffodilvarsity.edu.bd/web/image/op.course/243/image_1920&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;DataStructure-Vol-2-数组队列模拟-Array-Queues-Demo</summary>
      
    
    
    
    <category term="Java" scheme="https://johnnn765.github.io/categories/Java/"/>
    
    <category term="Data Structure" scheme="https://johnnn765.github.io/categories/Java/Data-Structure/"/>
    
    
    <category term="数据结构学习" scheme="https://johnnn765.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Java" scheme="https://johnnn765.github.io/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>DataStructure Vol.1 稀疏矩阵转换 Sparse Array Conversion</title>
    <link href="https://johnnn765.github.io/2022/10/08/DataStructure1/"/>
    <id>https://johnnn765.github.io/2022/10/08/DataStructure1/</id>
    <published>2022-10-08T04:01:18.000Z</published>
    <updated>2022-10-08T04:32:48.849Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://pd.daffodilvarsity.edu.bd/web/image/op.course/243/image_1920"></p><h1 id="DataStructure-Vol-1-稀疏矩阵转换-Sparse-Array-Conversion"><a href="#DataStructure-Vol-1-稀疏矩阵转换-Sparse-Array-Conversion" class="headerlink" title="DataStructure Vol.1 稀疏矩阵转换 Sparse Array Conversion"></a>DataStructure Vol.1 稀疏矩阵转换 Sparse Array Conversion</h1><p>稀疏矩阵的存储不宜用二维数组存储每个元素，那样的话会浪费很多的存储空间。所以可以使用一个一维数组存储其中的非零元素。这个一维数组的元素类型是一个三元组，由非零元素在该稀疏矩阵中的位置（行号和列号对）以及该元组的值构成。</p><h3 id="稀疏矩阵转换的原理示意图"><a href="#稀疏矩阵转换的原理示意图" class="headerlink" title="稀疏矩阵转换的原理示意图"></a>稀疏矩阵转换的原理示意图</h3><p><img src="https://img-blog.csdnimg.cn/20200920210043195.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21meDc3MDA0Mjk=,size_16,color_FFFFFF,t_70"></p><h3 id="Java实现代码"><a href="#Java实现代码" class="headerlink" title="Java实现代码"></a>Java实现代码</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><code class="hljs Java"><span class="hljs-keyword">import</span> java.util.ArrayList;<br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">SparseArray</span> &#123;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> &#123;<br>        <span class="hljs-comment">// 生成原始二维稀疏数组</span><br>        <span class="hljs-comment">// 0，1，2分别代表没有数据，数据类型A以及数据类型B</span><br><br>        <span class="hljs-type">int</span> chessArr1[][] = <span class="hljs-keyword">new</span> <span class="hljs-title class_">int</span>[<span class="hljs-number">10</span>][<span class="hljs-number">11</span>];<br>        <span class="hljs-comment">// 初始化11*11的0数值二维数组</span><br>        chessArr1[<span class="hljs-number">1</span>][<span class="hljs-number">2</span>] = <span class="hljs-number">1</span>;<br>        chessArr1[<span class="hljs-number">2</span>][<span class="hljs-number">3</span>] = <span class="hljs-number">2</span>;<br>        <span class="hljs-comment">// 给特定的位点进行赋值以生成二维稀疏数组</span><br>        <span class="hljs-comment">// 在实际应用中上述的二维稀疏数组为未知状态</span><br><br>        System.out.println(<span class="hljs-string">&quot;原始的二维系数数组为：&quot;</span>);<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span>[] row : chessArr1) &#123;<br>            <span class="hljs-comment">// 首先遍历二维稀疏矩阵中的每一行</span><br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> data : row) &#123;<br>                <span class="hljs-comment">// 然后遍历每一行中的每一个元素</span><br>                System.out.printf(<span class="hljs-string">&quot;%d\t&quot;</span>, data);<br>            &#125;<br>            System.out.println();<br>        &#125;<br>        <span class="hljs-comment">// 在终端打印出创建的二维稀疏数组</span><br><br>        <span class="hljs-comment">// 将二维稀疏数组转换为稀疏数组进行存储</span><br>        <span class="hljs-comment">// Step1：对稀疏数组进行遍历，得到非0元素的数量</span><br>        <span class="hljs-type">ArrayList</span> <span class="hljs-variable">row_list</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">ArrayList</span>&lt;&gt;();<br>        <span class="hljs-type">ArrayList</span> <span class="hljs-variable">col_list</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">ArrayList</span>&lt;&gt;();<br>        <span class="hljs-type">ArrayList</span> <span class="hljs-variable">val_list</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">ArrayList</span>&lt;&gt;();<br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; i &lt; chessArr1.length; i++) &#123;<br>            <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">j</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; j &lt; chessArr1[i].length; j++) &#123;<br>                <span class="hljs-keyword">if</span> (chessArr1[i][j] != <span class="hljs-number">0</span>) &#123;<br>                    row_list.add(i);<br>                    col_list.add(j);<br>                    val_list.add(chessArr1[i][j]);<br>                &#125;<br>            &#125;<br>        &#125;<br>        System.out.println(<span class="hljs-string">&quot;此二维稀疏数组中一共有&quot;</span>+ row_list.size() + <span class="hljs-string">&quot;个非零的值&quot;</span>);<br><br>        <span class="hljs-comment">// Step2：创建对应的存储用稀疏数组</span><br>        <span class="hljs-type">int</span> Compact_Array[][] = <span class="hljs-keyword">new</span> <span class="hljs-title class_">int</span>[row_list.size()+<span class="hljs-number">1</span>][<span class="hljs-number">3</span>];<br>        <span class="hljs-comment">// 创建存储用稀疏数组，数组第一行为头文件，记录原始数组的行列大小以及非0元素的个数</span><br>        <span class="hljs-comment">// 数组接下来每一行记录非0元素的坐标以及元素的值</span><br>        Compact_Array[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>] = chessArr1.length;<br>        Compact_Array[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>] = chessArr1[<span class="hljs-number">0</span>].length;<br>        Compact_Array[<span class="hljs-number">0</span>][<span class="hljs-number">2</span>] = row_list.size();<br>        <span class="hljs-comment">// 编写稀疏数组的头文件（第一行数据）</span><br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">ii</span> <span class="hljs-operator">=</span> <span class="hljs-number">1</span>; ii &lt; row_list.size()+<span class="hljs-number">1</span>; ii++) &#123;<br>            <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">jj</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; jj &lt; <span class="hljs-number">3</span>; jj++) &#123;<br>                <span class="hljs-keyword">if</span> (jj == <span class="hljs-number">0</span>) &#123;<br>                    Compact_Array[ii][jj] = (<span class="hljs-type">int</span>) row_list.get(ii-<span class="hljs-number">1</span>);<br>                &#125;<br>                <span class="hljs-keyword">if</span> (jj == <span class="hljs-number">1</span>) &#123;<br>                    Compact_Array[ii][jj] = (<span class="hljs-type">int</span>) col_list.get(ii-<span class="hljs-number">1</span>);<br>                &#125;<br>                <span class="hljs-keyword">if</span> (jj == <span class="hljs-number">2</span>) &#123;<br>                    Compact_Array[ii][jj] = (<span class="hljs-type">int</span>) val_list.get(ii-<span class="hljs-number">1</span>);<br>                &#125;<br>            &#125;<br>        &#125;<br>        <span class="hljs-comment">// 完成稀疏数组的赋值</span><br>        System.out.println(<span class="hljs-string">&quot;稀疏数组为：&quot;</span>);<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span>[] row : Compact_Array) &#123;<br>            <span class="hljs-comment">// 首先遍历二维稀疏矩阵中的每一行</span><br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> data : row) &#123;<br>                <span class="hljs-comment">// 然后遍历每一行中的每一个元素</span><br>                System.out.printf(<span class="hljs-string">&quot;%d\t&quot;</span>, data);<br>            &#125;<br>            System.out.println();<br>        &#125;<br>        <span class="hljs-comment">// 打印出赋值后的稀疏数组</span><br><br>        <span class="hljs-comment">// Step3：稀疏数组还原数据</span><br>        <span class="hljs-type">int</span> Recon_array[][] = <span class="hljs-keyword">new</span> <span class="hljs-title class_">int</span>[Compact_Array[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]][Compact_Array[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>]];<br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">iii</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; iii &lt; Compact_Array[<span class="hljs-number">0</span>][<span class="hljs-number">2</span>]; iii++) &#123;<br>            Recon_array[Compact_Array[iii+<span class="hljs-number">1</span>][<span class="hljs-number">0</span>]][Compact_Array[iii+<span class="hljs-number">1</span>][<span class="hljs-number">1</span>]] = Compact_Array[iii+<span class="hljs-number">1</span>][<span class="hljs-number">2</span>];<br>        &#125;<br>        <span class="hljs-comment">// 完成稀疏数组的还原</span><br>        System.out.println(<span class="hljs-string">&quot;还原的数组为：&quot;</span>);<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span>[] row : Recon_array) &#123;<br>            <span class="hljs-comment">// 首先遍历二维稀疏矩阵中的每一行</span><br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> data : row) &#123;<br>                <span class="hljs-comment">// 然后遍历每一行中的每一个元素</span><br>                System.out.printf(<span class="hljs-string">&quot;%d\t&quot;</span>, data);<br>            &#125;<br>            System.out.println();<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>__本文作者__：Johnnn765<br />__本文地址__： <a href="https://johnnn765.github.io/2022/10/08/DataStructure1/">https://johnnn765.github.io/2022/10/08/DataStructure1/</a> <br />__版权声明__：转载请注明出处！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://pd.daffodilvarsity.edu.bd/web/image/op.course/243/image_1920&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;DataStructure-Vol-1-稀疏矩阵转换-Sparse-Array-Conv</summary>
      
    
    
    
    <category term="Java" scheme="https://johnnn765.github.io/categories/Java/"/>
    
    <category term="Data Structure" scheme="https://johnnn765.github.io/categories/Java/Data-Structure/"/>
    
    
    <category term="数据结构学习" scheme="https://johnnn765.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Java" scheme="https://johnnn765.github.io/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow学习笔记 Vol.2</title>
    <link href="https://johnnn765.github.io/2022/10/08/Tensorflow2/"/>
    <id>https://johnnn765.github.io/2022/10/08/Tensorflow2/</id>
    <published>2022-10-08T02:48:58.000Z</published>
    <updated>2022-10-08T04:31:16.159Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.ytimg.com/vi/yjprpOoH5c8/maxresdefault.jpg"></p><h1 id="Tensorflow学习笔记-Vol-2"><a href="#Tensorflow学习笔记-Vol-2" class="headerlink" title="Tensorflow学习笔记 Vol.2"></a>Tensorflow学习笔记 Vol.2</h1><p>Tensorflow学习笔记2，Tensorflow的<strong>前向传播 (Foward Propagation) demo 案例</strong>，利用keras数据库中的MNIST数据集（MNIST数据集(Mixed National Institute of Standards and Technology database)是美国国家标准与技术研究院收集整理的大型手写数字数据库,包含60,000个示例的训练集以及10,000个示例的测试集）进行网络前向传播的演示。</p><h3 id="Python的代码实现"><a href="#Python的代码实现" class="headerlink" title="Python的代码实现"></a>Python的代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><span class="hljs-keyword">from</span> tensorflow <span class="hljs-keyword">import</span> keras<br><span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">import</span> os<br><br>os.environ[<span class="hljs-string">&#x27;TF_CPP_MIN_LOG_LEVEL&#x27;</span>] = <span class="hljs-string">&#x27;2&#x27;</span><br><span class="hljs-comment"># 运行程序的时候在头部屏蔽提醒信息仅打印错误信息</span><br><br>(x, y), _ = datasets.mnist.load_data()<br><span class="hljs-comment"># 从MNIST数据库中调取数据</span><br><span class="hljs-comment"># x的数据结构为：[60k, 28, 28]（60k张图片，每张图片的size为28*28）</span><br><span class="hljs-comment"># y的数据结构为：[60k]（60k个label）</span><br><br>x = tf.convert_to_tensor(x, dtype=tf.float32)<br>y = tf.convert_to_tensor(y, dtype=tf.int32)<br><span class="hljs-comment"># 将获取的数据转化为浮点数32数据类型的张量</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;张量x的形状与数据类型: &quot;</span>, x.shape, x.dtype)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;张量y的形状与数据类型: &quot;</span>, y.shape, y.dtype)<br><span class="hljs-comment"># 将张量的形状与数据类型打印至终端验证转换</span><br><br><span class="hljs-built_in">print</span>(tf.reduce_min(x), tf.reduce_max(x))<br><span class="hljs-built_in">print</span>(tf.reduce_min(y), tf.reduce_max(y))<br><span class="hljs-comment"># 在终端中打印张量x和张量y的最大值与最小值</span><br><span class="hljs-comment"># 张量x的最小值与最大值（0与255）代表图像的像素灰度值范围</span><br><span class="hljs-comment"># 张量y的最小值与最大值（0与9）代表图像标签0~9</span><br><br>x = x / <span class="hljs-number">255.</span><br><span class="hljs-built_in">print</span>(tf.reduce_min(x), tf.reduce_max(x))<br><span class="hljs-comment"># 将[0,255]的灰度值范围进行归一化操作</span><br><br>train_database = tf.data.Dataset.from_tensor_slices((x, y)).batch(<span class="hljs-number">128</span>)<br><span class="hljs-comment"># 由张量x与y创建训练集，每次从中抽取128个sample</span><br>train_iter = <span class="hljs-built_in">iter</span>(train_database)<br><span class="hljs-comment"># 创建迭代器，不断从训练集中每次抽取128个sample</span><br>sample = <span class="hljs-built_in">next</span>(train_iter)<br><span class="hljs-comment"># 利用next不断调用sample</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;batch:&#x27;</span>, sample[<span class="hljs-number">0</span>].shape, sample[<span class="hljs-number">1</span>].shape)<br><span class="hljs-comment"># 分别对应x和y的sample</span><br><br><span class="hljs-comment"># 对输入的张量x进行一系列将为的操作</span><br><span class="hljs-comment"># [b, 28*28(784)] =&gt; [b, 256] =&gt; [b, 128] =&gt; [b, 10]</span><br><span class="hljs-comment"># 权重系数张量的大小为[dim_in, dim_out]</span><br><span class="hljs-comment"># bias系数张量的大小为[dim_out]</span><br>w1 = tf.Variable(tf.random.truncated_normal([<span class="hljs-number">784</span>, <span class="hljs-number">256</span>], stddev = <span class="hljs-number">0.1</span>))<br>b1 = tf.Variable(tf.zeros([<span class="hljs-number">256</span>]))<br>w2 = tf.Variable(tf.random.truncated_normal([<span class="hljs-number">256</span>, <span class="hljs-number">128</span>], stddev = <span class="hljs-number">0.1</span>))<br>b2 = tf.Variable(tf.zeros([<span class="hljs-number">128</span>]))<br>w3 = tf.Variable(tf.random.truncated_normal([<span class="hljs-number">128</span>, <span class="hljs-number">10</span>], stddev = <span class="hljs-number">0.1</span>))<br>b3 = tf.Variable(tf.zeros([<span class="hljs-number">10</span>]))<br><span class="hljs-comment"># 指定标准方差优化参数初始化</span><br><br>learning_rate = <span class="hljs-number">1e-3</span><br><span class="hljs-comment"># 设置参数更新的学习率</span><br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br><span class="hljs-comment"># 将数据集进行10次循环</span><br>    <span class="hljs-keyword">for</span> step, (x, y) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_database):<br>    <span class="hljs-comment"># 对每一个batch进行循环</span><br>    <span class="hljs-comment"># 每循环一个step将数值进行打印</span><br>    <span class="hljs-comment"># x: [128, 28, 28] y: [128]</span><br>        x = tf.reshape(x, [-<span class="hljs-number">1</span>, <span class="hljs-number">28</span>*<span class="hljs-number">28</span>])<br>        <span class="hljs-comment"># 对张量x进行reshape操作[b, 28, 28] =&gt; [b, 784]</span><br><br>        <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:<br><br>            h1 = x@w1 + b1<br>            <span class="hljs-comment"># [b, 784]@[784, 256] +(Auto broadingcast) [256] =&gt; [b, 256]</span><br>            h1 = tf.nn.relu(h1)<br>            h2 = h1@w2 + b2<br>            <span class="hljs-comment"># [b, 256] =&gt; [b, 128]</span><br>            h2 = tf.nn.relu(h2)<br>            out_put = h2@w3 + b3<br>            <span class="hljs-comment"># [b, 128] =&gt; [b, 10]</span><br>            <span class="hljs-comment"># 中间层利用relu函数进行激活</span><br><br>            <span class="hljs-comment"># loss computing</span><br>            <span class="hljs-comment"># out_put的形状为[b, 10]</span><br>            <span class="hljs-comment"># 将y([b])转化为[b, 10]</span><br>            y_onehot = tf.one_hot(y, depth=<span class="hljs-number">10</span>)<br><br>            <span class="hljs-comment"># mse = mean(sym(y-out)^2)</span><br>            loss = tf.square(y_onehot-out_put)<br>            loss = tf.reduce_mean(loss)<br><br>        grad = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])<br>        <span class="hljs-comment"># 计算梯度</span><br>        w1.assign_sub(learning_rate * grad[<span class="hljs-number">0</span>])<br>        <span class="hljs-comment"># 使用assign_sub函数保持更新后的w1仍然为Variable类型不变</span><br>        b1.assign_sub(learning_rate * grad[<span class="hljs-number">1</span>])<br>        w2.assign_sub(learning_rate * grad[<span class="hljs-number">2</span>])<br>        b2.assign_sub(learning_rate * grad[<span class="hljs-number">3</span>])<br>        w3.assign_sub(learning_rate * grad[<span class="hljs-number">4</span>])<br>        b3.assign_sub(learning_rate * grad[<span class="hljs-number">5</span>])<br>        <span class="hljs-comment"># 利用计算的梯度和指定的学习率进行参数的更新</span><br><br>        <span class="hljs-keyword">if</span> step % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(epoch, step, <span class="hljs-string">&#x27;loss:&#x27;</span>, <span class="hljs-built_in">float</span>(loss))<br></code></pre></td></tr></table></figure><p>__本文作者__：Johnnn765<br />__本文地址__： <a href="https://johnnn765.github.io/2022/10/08/Tensorflow2/">https://johnnn765.github.io/2022/10/08/Tensorflow2/</a> <br />__版权声明__：转载请注明出处！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://i.ytimg.com/vi/yjprpOoH5c8/maxresdefault.jpg&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;Tensorflow学习笔记-Vol-2&quot;&gt;&lt;a href=&quot;#Tensorflow学习笔记-Vol-2&quot; class=</summary>
      
    
    
    
    <category term="Python" scheme="https://johnnn765.github.io/categories/Python/"/>
    
    <category term="Tensorflow" scheme="https://johnnn765.github.io/categories/Python/Tensorflow/"/>
    
    
    <category term="TF学习" scheme="https://johnnn765.github.io/tags/TF%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Python" scheme="https://johnnn765.github.io/tags/Python/"/>
    
    <category term="深度学习" scheme="https://johnnn765.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow学习笔记 Vol.1</title>
    <link href="https://johnnn765.github.io/2022/10/07/Tensorflow1/"/>
    <id>https://johnnn765.github.io/2022/10/07/Tensorflow1/</id>
    <published>2022-10-07T10:25:55.000Z</published>
    <updated>2022-10-07T11:24:13.437Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.ytimg.com/vi/yjprpOoH5c8/maxresdefault.jpg"></p><h1 id="Tensorflow学习笔记-Vol-1"><a href="#Tensorflow学习笔记-Vol-1" class="headerlink" title="Tensorflow学习笔记 Vol.1"></a>Tensorflow学习笔记 Vol.1</h1><p>Tensorflow学习笔记1，Tensorflow的基础操作，包括<strong>数据类型</strong>，<strong>张量的创建</strong>，<strong>损失计算</strong>，<strong>全网络创建</strong>，<strong>索引与切片</strong>，<strong>维度变换</strong>，<strong>广播机制</strong>以及<strong>基础数学运算</strong></p><h3 id="1-TF的数据类型"><a href="#1-TF的数据类型" class="headerlink" title="1.TF的数据类型"></a>1.TF的数据类型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><br>aa = tf.constant(<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(aa)<br><span class="hljs-comment"># int32</span><br><br>aa = tf.constant(<span class="hljs-number">1.</span>)<br><span class="hljs-built_in">print</span>(aa)<br><span class="hljs-comment"># float32</span><br><br><span class="hljs-comment"># aa = tf.constant(2.2, dtype=tf.int32) Convert failed</span><br><br>aa = tf.constant(<span class="hljs-number">2.</span>, dtype=tf.double)<br><span class="hljs-built_in">print</span>(aa)<br><span class="hljs-comment"># float64</span><br><br>aa = tf.constant([<span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>])<br><span class="hljs-built_in">print</span>(aa)<br><span class="hljs-comment"># bool</span><br><br>aa = tf.constant(<span class="hljs-string">&#x27;Hello, world.&#x27;</span>)<br><span class="hljs-built_in">print</span>(aa)<br><span class="hljs-comment"># string</span><br><br>b = tf.constant([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])<br>bb = tf.cast(b, dtype=tf.<span class="hljs-built_in">bool</span>)<br>bbb = tf.cast(bb, dtype=tf.int32)<br><span class="hljs-built_in">print</span>(b)<br><span class="hljs-built_in">print</span>(bb)<br><span class="hljs-built_in">print</span>(bbb)<br></code></pre></td></tr></table></figure><h3 id="2-张量的创建"><a href="#2-张量的创建" class="headerlink" title="2.张量的创建"></a>2.张量的创建</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> numpy <span class="hljs-keyword">import</span> int32<br><br>a = np.ones([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>(a)<br><span class="hljs-comment"># a: numpy</span><br>aa = tf.convert_to_tensor(a, dtype=int32)<br><span class="hljs-built_in">print</span>(aa)<br><span class="hljs-comment"># aa: tensor converted from numpy</span><br><span class="hljs-comment"># Create tensor from numpy</span><br><br><br>b = tf.zeros([])<br><span class="hljs-built_in">print</span>(b)<br><span class="hljs-comment"># Create tensor from tf.zeros and tf.ones; [] denotes the shape of the tensor</span><br><br>c = tf.zeros([<span class="hljs-number">1</span>])<br><span class="hljs-built_in">print</span>(c)<br><br>d = tf.zeros([<span class="hljs-number">3</span>, <span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>(d)<br><br>e = tf.ones([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>(e)<br><br><br>a = np.ones([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br><span class="hljs-comment"># a: numpy</span><br>aa = tf.convert_to_tensor(a, dtype=int32)<br>f = tf.zeros_like(aa)<br><span class="hljs-built_in">print</span>(f)<br><span class="hljs-comment"># Create zeros tensor with zeros_like to copy the size of an existing tensor</span><br><br>g = tf.zeros(aa.shape)<br><span class="hljs-built_in">print</span>(f)<br><span class="hljs-comment"># tf.zeros_like(aa) is equal to tf.zeros(aa.shape)</span><br><br><br>h = tf.fill([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>], <span class="hljs-number">5</span>)<br><span class="hljs-built_in">print</span>(h)<br><span class="hljs-comment"># create tensor with same elements filling inside a defined shape using tf.fill</span><br>h = tf.fill([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>], <span class="hljs-number">5.</span>)<br><span class="hljs-built_in">print</span>(h)<br><span class="hljs-comment"># different dtype when creating the tensor</span><br><br><br>i = tf.random.normal([<span class="hljs-number">3</span>, <span class="hljs-number">3</span>], mean=<span class="hljs-number">0</span>, stddev=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(i)<br><span class="hljs-comment"># create normal distribution initial tensor with tf.random.normal with mean and standard devision tunnable</span><br><br>i = tf.random.truncated_normal([<span class="hljs-number">3</span>, <span class="hljs-number">3</span>], mean=<span class="hljs-number">0</span>, stddev=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(i)<br><span class="hljs-comment"># use truncated_normal instead of normal to prevent gradient vanish and improve initilize performance</span><br><br>i = tf.random.uniform([<span class="hljs-number">3</span>, <span class="hljs-number">3</span>], minval=<span class="hljs-number">0</span>, maxval=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(i)<br><span class="hljs-comment"># create uniform distributed random tensor with random.uniform</span><br><br>i = tf.random.uniform([<span class="hljs-number">3</span>, <span class="hljs-number">3</span>], minval=<span class="hljs-number">0</span>, maxval=<span class="hljs-number">100</span>, dtype=int32)<br><span class="hljs-built_in">print</span>(i)<br><br><br>index_original = tf.<span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>)<br><span class="hljs-comment"># create 0~10 ordered tensor with tf.range</span><br>index_shuffled = tf.random.shuffle(index_original)<br><span class="hljs-comment"># random shuffle the ordered tensor with random.shuffle</span><br><span class="hljs-built_in">print</span>(index_original)<br><span class="hljs-built_in">print</span>(index_shuffled)<br>a_sequence = tf.random.normal([<span class="hljs-number">10</span>, <span class="hljs-number">3</span>])<br>b_sequence = tf.random.uniform([<span class="hljs-number">10</span>], maxval=<span class="hljs-number">10</span>, dtype=tf.int32)<br><span class="hljs-built_in">print</span>(a_sequence)<br><span class="hljs-built_in">print</span>(b_sequence)<br>a_sequence_shuffled = tf.gather(a_sequence, index_shuffled)<br>b_sequence_shuffled = tf.gather(b_sequence, index_shuffled)<br><span class="hljs-built_in">print</span>(a_sequence_shuffled)<br><span class="hljs-built_in">print</span>(b_sequence_shuffled)<br><span class="hljs-comment"># a_sequence_shuffled and b_sequence_shuffled has same correspondance with tf.gather</span><br></code></pre></td></tr></table></figure><h3 id="3-损失计算"><a href="#3-损失计算" class="headerlink" title="3.损失计算"></a>3.损失计算</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> numpy <span class="hljs-keyword">import</span> int32<br><br>out = tf.random.uniform([<span class="hljs-number">4</span>, <span class="hljs-number">10</span>])<br><span class="hljs-comment"># 4 pictures each with 10 elements</span><br><span class="hljs-built_in">print</span>(out)<br><br>y = tf.<span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>)<br><span class="hljs-comment"># create labels for the 4 pictures</span><br>y = tf.one_hot(y, depth=<span class="hljs-number">10</span>)<br><span class="hljs-built_in">print</span>(y)<br><span class="hljs-comment"># encode y with depth equals to 10 using tf.one_hot</span><br><br>loss = tf.keras.losses.mse(y, out)<br><span class="hljs-comment"># calculate the mse of each batch of the input &quot;out&quot;</span><br><span class="hljs-built_in">print</span>(loss)<br>loss = tf.reduce_mean(loss)<br><span class="hljs-comment"># mean loss of the mse from all the batches</span><br><span class="hljs-built_in">print</span>(loss)<br><span class="hljs-comment"># Data type: scalar</span><br></code></pre></td></tr></table></figure><h3 id="4-全网络创建"><a href="#4-全网络创建" class="headerlink" title="4.全网络创建"></a>4.全网络创建</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><span class="hljs-keyword">from</span> keras.applications.densenet <span class="hljs-keyword">import</span> layers<br><br><span class="hljs-comment"># x @ w + b</span><br>net = layers.Dense(<span class="hljs-number">10</span>)<br><span class="hljs-comment"># net is factor b</span><br>net.build((<span class="hljs-number">4</span>, <span class="hljs-number">8</span>))<br><span class="hljs-comment"># define the shape of the factor x</span><br><span class="hljs-built_in">print</span>(net.kernel)<br><span class="hljs-comment"># kernel is the factor w, which has the shape of (8, 10)</span><br><span class="hljs-built_in">print</span>(net.bias)<br><span class="hljs-comment"># bias has the shape of (1, 10)</span><br><br><span class="hljs-comment"># x @ w + b</span><br>x = tf.random.normal([<span class="hljs-number">4</span>, <span class="hljs-number">784</span>])<br><span class="hljs-comment"># create factor x which has a shape of (4, 784)</span><br><br>net = layers.Dense(<span class="hljs-number">10</span>)<br>net.build((<span class="hljs-number">4</span>, <span class="hljs-number">784</span>))<br><span class="hljs-built_in">print</span>(net(x).shape)<br><span class="hljs-comment"># input x into the net and output (x @ w) has a shape of (4, 10)</span><br><span class="hljs-built_in">print</span>(net.kernel.shape)<br><span class="hljs-comment"># kernel (w factor) has a shape of (784, 10)</span><br><span class="hljs-built_in">print</span>(net.bias.shape)<br><span class="hljs-comment"># bias (factor b) has a shape of (1, 10)</span><br></code></pre></td></tr></table></figure><h3 id="5-张量的基础索引"><a href="#5-张量的基础索引" class="headerlink" title="5.张量的基础索引"></a>5.张量的基础索引</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><br>a = tf.ones([<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>(a[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>])<br><span class="hljs-built_in">print</span>(a[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>][<span class="hljs-number">0</span>])<br><span class="hljs-built_in">print</span>(a[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>][<span class="hljs-number">0</span>][<span class="hljs-number">2</span>])<br><span class="hljs-comment"># basic indexing with tensor[]</span><br><br><br>a = tf.random.normal([<span class="hljs-number">4</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>(a[<span class="hljs-number">1</span>].shape)<br><span class="hljs-built_in">print</span>(a[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>].shape)<br><span class="hljs-built_in">print</span>(a[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>].shape)<br><span class="hljs-built_in">print</span>(a[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>].shape)<br><span class="hljs-comment"># Numpy-style indexing (recommended)</span><br><br><br>a = tf.<span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>)<br><span class="hljs-comment"># left-to-right label start from 0; right-to-left label start from -1</span><br><span class="hljs-comment"># start : end (start is included while end is excluded)</span><br><span class="hljs-comment"># blank (2:?) means goes all way down to the end</span><br><span class="hljs-built_in">print</span>(a)<br><span class="hljs-built_in">print</span>(a[-<span class="hljs-number">1</span>:])<br><span class="hljs-built_in">print</span>(a[-<span class="hljs-number">3</span>:-<span class="hljs-number">1</span>])<br><span class="hljs-built_in">print</span>(a[:<span class="hljs-number">2</span>])<br><span class="hljs-built_in">print</span>(a[:-<span class="hljs-number">1</span>])<br><span class="hljs-built_in">print</span>(a[:])<br><br><br><span class="hljs-comment"># indexing using :</span><br>a = tf.random.normal([<span class="hljs-number">4</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>(a.shape)<br><span class="hljs-built_in">print</span>(a[<span class="hljs-number">0</span>].shape)<br><span class="hljs-built_in">print</span>(a[<span class="hljs-number">0</span>, :, :, :].shape)<br><span class="hljs-built_in">print</span>(a[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, :, :].shape)<br><span class="hljs-built_in">print</span>(a[:, :, :, <span class="hljs-number">0</span>].shape)<br><span class="hljs-built_in">print</span>(a[:, <span class="hljs-number">0</span>, :, :].shape)<br><span class="hljs-comment"># : means all elements is required</span><br><br><br><span class="hljs-comment"># indexing using :: (start:end:step)</span><br>a = tf.random.normal([<span class="hljs-number">4</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>(a.shape)<br><span class="hljs-built_in">print</span>(a[<span class="hljs-number">0</span>:<span class="hljs-number">4</span>:<span class="hljs-number">5</span>, :, :, :].shape)<br><span class="hljs-built_in">print</span>(a[:, ::<span class="hljs-number">2</span>, ::<span class="hljs-number">2</span>, :].shape)<br><br><br><span class="hljs-comment"># indexing using ::-? (reverse indexing)</span><br>a = tf.<span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>)<br><span class="hljs-built_in">print</span>(a)<br><span class="hljs-built_in">print</span>(a[::-<span class="hljs-number">1</span>])<br><span class="hljs-built_in">print</span>(a[<span class="hljs-number">3</span>::-<span class="hljs-number">1</span>])<br><span class="hljs-built_in">print</span>(a[<span class="hljs-number">2</span>::-<span class="hljs-number">1</span>])<br><span class="hljs-built_in">print</span>(a[<span class="hljs-number">1</span>::-<span class="hljs-number">1</span>])<br><span class="hljs-built_in">print</span>(a[<span class="hljs-number">0</span>::-<span class="hljs-number">1</span>])<br><br><br><span class="hljs-comment"># indexing using ...</span><br>a = tf.random.normal([<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>(a[<span class="hljs-number">0</span>].shape)<br><span class="hljs-built_in">print</span>(a[<span class="hljs-number">0</span>, :, :, :, :].shape)<br><span class="hljs-built_in">print</span>(a[<span class="hljs-number">0</span>, ...].shape)<br><span class="hljs-built_in">print</span>(a[..., <span class="hljs-number">0</span>].shape)<br><span class="hljs-built_in">print</span>(a[<span class="hljs-number">0</span>, ..., <span class="hljs-number">2</span>].shape)<br><span class="hljs-built_in">print</span>(a[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>].shape)<br></code></pre></td></tr></table></figure><h3 id="6-张量的灵活索引"><a href="#6-张量的灵活索引" class="headerlink" title="6.张量的灵活索引"></a>6.张量的灵活索引</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><br><span class="hljs-comment"># data: [classes, students, subjects]</span><br>data = tf.random.normal([<span class="hljs-number">4</span>, <span class="hljs-number">35</span>, <span class="hljs-number">8</span>])<br>a = tf.gather(data, axis=<span class="hljs-number">0</span>, indices=[<span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br><span class="hljs-comment"># gather class data of class 2 and 3</span><br><span class="hljs-built_in">print</span>(a.shape)<br>b = tf.gather(data, axis=<span class="hljs-number">0</span>, indices=[<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>])<br><span class="hljs-comment"># gather class data in the order of 2-1-3-0, which can not be achieved by using :</span><br><span class="hljs-built_in">print</span>(b.shape)<br>c = tf.gather(data, axis=<span class="hljs-number">1</span>, indices=[<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>, <span class="hljs-number">16</span>])<br><span class="hljs-comment"># gather students 2, 3, 7, 9 and 16 from all 4 classes</span><br><span class="hljs-built_in">print</span>(c.shape)<br>d = tf.gather(data, axis=<span class="hljs-number">2</span>, indices=[<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">7</span>])<br><span class="hljs-comment"># gather subject 2, 3 and 7 data of all students from all four classes</span><br><span class="hljs-built_in">print</span>(d.shape)<br><br><br>data = tf.random.normal([<span class="hljs-number">4</span>, <span class="hljs-number">35</span>, <span class="hljs-number">8</span>])<br>a = tf.gather(data, axis=<span class="hljs-number">1</span>, indices=[<span class="hljs-number">5</span>, <span class="hljs-number">7</span>])<br><span class="hljs-comment"># gather two students from each class</span><br><span class="hljs-built_in">print</span>(a.shape)<br>b = tf.gather(a, axis=<span class="hljs-number">2</span>, indices=[<span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br><span class="hljs-comment">#futher gather two subjects from the modified data</span><br><span class="hljs-built_in">print</span>(b.shape)<br><br><br><span class="hljs-comment"># tf.gather can only gather data from a single dimension</span><br><span class="hljs-comment"># tf.gather_nd can gather multiple dimension</span><br>data = tf.random.normal([<span class="hljs-number">4</span>, <span class="hljs-number">35</span>, <span class="hljs-number">8</span>])<br>a = tf.gather_nd(data, [<span class="hljs-number">0</span>])<br><span class="hljs-comment"># gather class 0 data</span><br><span class="hljs-built_in">print</span>(a.shape)<br><span class="hljs-built_in">print</span>(data[<span class="hljs-number">0</span>].shape)<br>b = tf.gather_nd(data, [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])<br><span class="hljs-comment"># gather class 0 student 1 data</span><br><span class="hljs-built_in">print</span>(b.shape)<br><span class="hljs-built_in">print</span>(data[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>].shape)<br>c = tf.gather_nd(data, [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>])<br><span class="hljs-comment"># gather class 0 student 1 subject 2 data</span><br><span class="hljs-built_in">print</span>(c.shape)<br><span class="hljs-built_in">print</span>(data[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>].shape)<br>d = tf.gather_nd(data, [[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>]])<br><span class="hljs-comment"># return in form of vector</span><br><span class="hljs-built_in">print</span>(d.shape)<br><br><br>data = tf.random.normal([<span class="hljs-number">4</span>, <span class="hljs-number">35</span>, <span class="hljs-number">8</span>])<br>a = tf.gather_nd(data, [[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]])<br><span class="hljs-comment"># gather data of class1_student1 (shape=[8]) and class2_student2</span><br><span class="hljs-built_in">print</span>(a.shape)<br>b = tf.gather_nd(data, [[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]])<br><span class="hljs-comment"># gather data of class1_student1_subject1, ... and class3_student3_subject3 (shape=[])</span><br><span class="hljs-built_in">print</span>(b.shape)<br><br><br><span class="hljs-comment"># indexing using true/false (tf.boolean_mask)</span><br>data = tf.random.normal([<span class="hljs-number">4</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">3</span>])<br>a = tf.boolean_mask(data, mask=[<span class="hljs-literal">True</span>, <span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>, <span class="hljs-literal">True</span>])<br><span class="hljs-comment"># default mask dimension is dimension 0</span><br><span class="hljs-built_in">print</span>(a.shape)<br>b = tf.boolean_mask(data, mask=[<span class="hljs-literal">True</span>, <span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>], axis=<span class="hljs-number">3</span>)<br><span class="hljs-comment"># with defined axis</span><br><span class="hljs-built_in">print</span>(b.shape)<br>data = tf.ones([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])<br>c = tf.boolean_mask(data, mask=[[<span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>, <span class="hljs-literal">False</span>], [<span class="hljs-literal">False</span>, <span class="hljs-literal">True</span>, <span class="hljs-literal">True</span>]])<br><span class="hljs-built_in">print</span>(c.shape)<br></code></pre></td></tr></table></figure><h3 id="8-张量的维度变换"><a href="#8-张量的维度变换" class="headerlink" title="8.张量的维度变换"></a>8.张量的维度变换</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><br><span class="hljs-comment"># reshape tensor with tf.reshape</span><br>data = tf.random.normal([<span class="hljs-number">4</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>(data.shape)<br><span class="hljs-built_in">print</span>(data.ndim)<br>a = tf.reshape(data, [<span class="hljs-number">4</span>, <span class="hljs-number">28</span>*<span class="hljs-number">28</span>, <span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>(a.shape)<br><span class="hljs-built_in">print</span>(a.ndim)<br>b = tf.reshape(data, [<span class="hljs-number">4</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">3</span>])<br><span class="hljs-comment"># &quot;-1&quot; can be auto calculated by system (only single &quot;-1&quot; is allowed)</span><br><span class="hljs-built_in">print</span>(b.shape)<br><span class="hljs-built_in">print</span>(b.ndim)<br>c = tf.reshape(data, [<span class="hljs-number">4</span>, <span class="hljs-number">28</span>*<span class="hljs-number">28</span>*<span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>(c.shape)<br><span class="hljs-built_in">print</span>(c.ndim)<br>d = tf.reshape(data, [<span class="hljs-number">4</span>, -<span class="hljs-number">1</span>])<br><span class="hljs-comment"># &quot;-1&quot; can be auto calculated by system (only single &quot;-1&quot; is allowed)</span><br><span class="hljs-built_in">print</span>(d.shape)<br><span class="hljs-built_in">print</span>(d.ndim)<br><br><br>data = tf.random.normal([<span class="hljs-number">4</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>(data.shape)<br><span class="hljs-built_in">print</span>(data.ndim)<br>a = tf.reshape(tf.reshape(data, [<span class="hljs-number">4</span>, -<span class="hljs-number">1</span>]), [<span class="hljs-number">4</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>(a.shape)<br><span class="hljs-built_in">print</span>(a.ndim)<br>b = tf.reshape(tf.reshape(data, [<span class="hljs-number">4</span>, -<span class="hljs-number">1</span>]), [<span class="hljs-number">4</span>, <span class="hljs-number">14</span>, <span class="hljs-number">56</span>, <span class="hljs-number">3</span>])<br><span class="hljs-comment"># reshape into different shape while preserving same ndim</span><br><span class="hljs-built_in">print</span>(b.shape)<br><span class="hljs-built_in">print</span>(b.ndim)<br>c = tf.reshape(tf.reshape(a, [<span class="hljs-number">4</span>, -<span class="hljs-number">1</span>]), [<span class="hljs-number">4</span>, <span class="hljs-number">1</span>, <span class="hljs-number">784</span>, <span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>(c.shape)<br><span class="hljs-built_in">print</span>(c.ndim)<br><br><br><span class="hljs-comment"># transpose tensor content with tf.transpose</span><br>data = tf.random.normal([<span class="hljs-number">4</span>, <span class="hljs-number">18</span>, <span class="hljs-number">28</span>, <span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>(data.shape)<br>a = tf.transpose(data)<br><span class="hljs-built_in">print</span>(a.shape)<br>b = tf.transpose(data, perm=[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>])<br><span class="hljs-comment"># use &quot;perm&quot; input factor to define transpose order</span><br><span class="hljs-built_in">print</span>(b.shape)<br><br><br><span class="hljs-comment"># tensor dim expansion with tf.expand_dims</span><br>data = tf.random.normal([<span class="hljs-number">4</span>, <span class="hljs-number">35</span>, <span class="hljs-number">8</span>])<br>a = tf.expand_dims(data, axis=<span class="hljs-number">0</span>)<br><span class="hljs-comment"># define dim expansion position with input factor &quot;axis&quot;</span><br><span class="hljs-built_in">print</span>(a.shape)<br>b = tf.expand_dims(data, axis=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(b.shape)<br>c = tf.expand_dims(data, axis=<span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(c.shape)<br>d = tf.expand_dims(data, axis=-<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(d.shape)<br>d = tf.expand_dims(data, axis=-<span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(d.shape)<br><br><br><span class="hljs-comment"># dim squeeze with tf.squeeze to eliminate 1 dimensions</span><br>data = tf.random.normal([<span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">8</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>(data.shape)<br>a = tf.squeeze(data)<br><span class="hljs-built_in">print</span>(a.shape)<br>b = tf.squeeze(data, axis=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(b.shape)<br>c = tf.squeeze(data, axis=<span class="hljs-number">3</span>)<br><span class="hljs-built_in">print</span>(c.shape)<br>d = tf.squeeze(data, axis=-<span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(d.shape)<br></code></pre></td></tr></table></figure><h3 id="9-广播机制"><a href="#9-广播机制" class="headerlink" title="9.广播机制"></a>9.广播机制</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><br>x = tf.random.normal([<span class="hljs-number">4</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>((x+tf.random.normal([<span class="hljs-number">3</span>])).shape)<br><span class="hljs-comment"># auto broadcasting without using tf.broadcasting_nd</span><br><span class="hljs-built_in">print</span>((x+tf.random.normal([<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">1</span>])).shape)<br><span class="hljs-built_in">print</span>((x+tf.random.normal([<span class="hljs-number">4</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])).shape)<br><span class="hljs-comment"># print((x+tf.random.normal([1, 4, 1, 1])).shape)</span><br><span class="hljs-comment"># cannot auto broadcasting due to &quot;32&quot; and &quot;4&quot;</span><br><br>b = tf.broadcast_to(tf.random.normal([<span class="hljs-number">4</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]), [<span class="hljs-number">4</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>(b.shape)<br><br><br><span class="hljs-comment"># broadcast and tile</span><br>a = tf.ones([<span class="hljs-number">3</span>, <span class="hljs-number">4</span>])<br>a1 = tf.broadcast_to(a, [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])<br><span class="hljs-comment"># dimension expansion with tf.broadcast_to</span><br><span class="hljs-built_in">print</span>(a1.shape)<br><br>a2 = tf.expand_dims(a, axis=<span class="hljs-number">0</span>)<br><span class="hljs-comment"># step1: expand dimension with tf.expand_dims</span><br>a2 = tf.tile(a2, [<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br><span class="hljs-comment"># step2: copy to defined dimentions with tf.tile</span><br><span class="hljs-built_in">print</span>(a2.shape)<br></code></pre></td></tr></table></figure><h3 id="10-TF基础数学运算"><a href="#10-TF基础数学运算" class="headerlink" title="10.TF基础数学运算"></a>10.TF基础数学运算</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br>a = tf.ones([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>])<br>b = tf.fill([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], <span class="hljs-number">2.0</span>)<br><span class="hljs-comment"># Use tf.fill to generate matrix filled with desired elements</span><br><br>a+b, a-b, a*b, a/b<br><span class="hljs-comment"># 张量中对应元素之间的【分别】数学运算</span><br><br>b//a<br><span class="hljs-comment"># 利用//进行整除数学操作</span><br><br>a = tf.constant([<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>])<br>b = tf.constant([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>(b%a)<br><span class="hljs-built_in">print</span>(tf.math.mod(b, a))<br><span class="hljs-comment"># 两种方法计算取余数的方法</span><br><br>a = tf.ones([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>])<br>tf.math.log(a)<br><span class="hljs-comment"># 计算loge(a)</span><br><br>tf.exp(a)<br><span class="hljs-comment"># e的a次方</span><br><br>tf.math.log(<span class="hljs-number">8.</span>)/tf.math.log(<span class="hljs-number">2.</span>)<br><span class="hljs-comment"># 用换底的方法计算log2(8)</span><br><br>tf.math.log(<span class="hljs-number">100.</span>)/tf.math.log(<span class="hljs-number">10.</span>)<br><span class="hljs-comment"># 用换底的方法计算log10(100)</span><br><br><span class="hljs-built_in">print</span>(tf.<span class="hljs-built_in">pow</span>(b, <span class="hljs-number">3</span>))<br><span class="hljs-built_in">print</span>(b**<span class="hljs-number">3</span>)<br><span class="hljs-comment"># 两种方法进行次方计算</span><br><br>b = tf.fill([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], <span class="hljs-number">2.0</span>)<br>tf.math.sqrt(b)<br><span class="hljs-comment"># 利用 tf.math.sqrt进行开方运算</span><br><br>a = tf.ones([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>])<br>b = tf.fill([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], <span class="hljs-number">2.0</span>)<br><span class="hljs-built_in">print</span>(a@b)<br><span class="hljs-built_in">print</span>(tf.matmul(a, b))<br><span class="hljs-comment"># 两种方法进行矩阵之间的相乘</span><br><br>a = tf.ones([<span class="hljs-number">4</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br>b = tf.fill([<span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>], <span class="hljs-number">2.</span>)<br><span class="hljs-built_in">print</span>(a@b)<br><span class="hljs-built_in">print</span>(tf.matmul(a, b))<br><span class="hljs-comment"># tensorflow默认将后两个维度当成矩阵进行相乘运算</span><br><br>a = tf.ones([<span class="hljs-number">4</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br>b = tf.fill([<span class="hljs-number">3</span>, <span class="hljs-number">5</span>], <span class="hljs-number">2.</span>)<br>bb = tf.broadcast_to(b, [<span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>])<br><span class="hljs-built_in">print</span>(a@bb)<br><span class="hljs-built_in">print</span>(tf.matmul(a, bb))<br><span class="hljs-comment"># tensorflow默认将后两个维度当成矩阵进行相乘运算</span><br><span class="hljs-comment"># 利用tf.broadcast_to将不满足维度要求的张量扩展到可以进行相乘的维度</span><br><br>X = tf.ones([<span class="hljs-number">4</span>, <span class="hljs-number">2</span>])<br>W = tf.ones([<span class="hljs-number">2</span>, <span class="hljs-number">1</span>])<br>b = tf.constant(<span class="hljs-number">0.1</span>)<br>out = X@W+b<br><span class="hljs-built_in">print</span>(tf.nn.relu(out))<br><span class="hljs-comment"># out = relu(X@W + b)</span><br></code></pre></td></tr></table></figure><p>__本文作者__：Johnnn765<br />__本文地址__： <a href="https://johnnn765.github.io/2022/10/07/Tensorflow1/">https://johnnn765.github.io/2022/10/07/Tensorflow1/</a> <br />__版权声明__：转载请注明出处！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://i.ytimg.com/vi/yjprpOoH5c8/maxresdefault.jpg&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;Tensorflow学习笔记-Vol-1&quot;&gt;&lt;a href=&quot;#Tensorflow学习笔记-Vol-1&quot; class=</summary>
      
    
    
    
    <category term="Python" scheme="https://johnnn765.github.io/categories/Python/"/>
    
    <category term="Tensorflow" scheme="https://johnnn765.github.io/categories/Python/Tensorflow/"/>
    
    
    <category term="TF学习" scheme="https://johnnn765.github.io/tags/TF%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Python" scheme="https://johnnn765.github.io/tags/Python/"/>
    
    <category term="深度学习" scheme="https://johnnn765.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
